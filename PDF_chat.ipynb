{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The author of the book is Reid Hoffman.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS \n",
    "os.environ['OPENAI_API_KEY']=\"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\"\n",
    "\n",
    "\n",
    "doc_reader = PdfReader('impromptu-rh.pdf')\n",
    "\n",
    "raw_text = ''\n",
    "for i, page in enumerate(doc_reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200, #striding over the text\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = FAISS.from_texts(texts, embeddings)\n",
    "query = \"how does GPT-4 change social media?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type='stuff')\n",
    "\n",
    "query = \"Who is the author of the book?\"\n",
    "\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is the author of the book?\"\n",
    "query_02 = \"Has it rained this week?\"\n",
    "docs = docsearch.similarity_search(query_02)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The book is called Philosophy and the Public by di Cesare.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Name of the book?'\n",
    "docs = docsearch.similarity_search(query, k=4)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the chain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:303: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output:  OpenAI is a research laboratory based in San Francisco, California, founded in 2015 by Elon Musk, Sam Altman, Greg Brockman, and Ilya Sutskever. Score: 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\veace\\Desktop\\Langchain_S\\PDF_chat.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/veace/Desktop/Langchain_S/PDF_chat.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwho are openai?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/veace/Desktop/Langchain_S/PDF_chat.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m docs \u001b[39m=\u001b[39m docsearch\u001b[39m.\u001b[39msimilarity_search(query,k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/veace/Desktop/Langchain_S/PDF_chat.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m=\u001b[39m chain({\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/veace/Desktop/Langchain_S/PDF_chat.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m results\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    170\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[0;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    156\u001b[0m     inputs,\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[0;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\combine_documents\\map_rerank.py:100\u001b[0m, in \u001b[0;36mMapRerankDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[0;32m     94\u001b[0m     \u001b[39mself\u001b[39m, docs: List[Document], callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m     96\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine documents in a map rerank manner.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[39m    Combine by mapping first chain over all documents, then reranking the results.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mapply_and_parse(\n\u001b[0;32m    101\u001b[0m         \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    102\u001b[0m         [{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_variable_name: d\u001b[39m.\u001b[39;49mpage_content}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs} \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m docs],\n\u001b[0;32m    103\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_results(docs, results)\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:308\u001b[0m, in \u001b[0;36mLLMChain.apply_and_parse\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    303\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    304\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe apply_and_parse method is deprecated, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstead pass an output parser directly to LLMChain.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m )\n\u001b[0;32m    307\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(input_list, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_generation(result)\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:314\u001b[0m, in \u001b[0;36mLLMChain._parse_generation\u001b[1;34m(self, generation)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_parse_generation\u001b[39m(\n\u001b[0;32m    311\u001b[0m     \u001b[39mself\u001b[39m, generation: List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]\n\u001b[0;32m    312\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]:\n\u001b[0;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39moutput_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m         \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    315\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(res[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_key])\n\u001b[0;32m    316\u001b[0m             \u001b[39mfor\u001b[39;49;00m res \u001b[39min\u001b[39;49;00m generation\n\u001b[0;32m    317\u001b[0m         ]\n\u001b[0;32m    318\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:315\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_parse_generation\u001b[39m(\n\u001b[0;32m    311\u001b[0m     \u001b[39mself\u001b[39m, generation: List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]\n\u001b[0;32m    312\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]:\n\u001b[0;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39moutput_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m         \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 315\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(res[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_key])\n\u001b[0;32m    316\u001b[0m             \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m generation\n\u001b[0;32m    317\u001b[0m         ]\n\u001b[0;32m    318\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\n",
      "File \u001b[1;32mc:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\output_parsers\\regex.py:28\u001b[0m, in \u001b[0;36mRegexParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_output_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse output: \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     31\u001b[0m             key: text \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_output_key \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m             \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys\n\u001b[0;32m     33\u001b[0m         }\n",
      "\u001b[1;31mValueError\u001b[0m: Could not parse output:  OpenAI is a research laboratory based in San Francisco, California, founded in 2015 by Elon Musk, Sam Altman, Greg Brockman, and Ilya Sutskever. Score: 100"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(), \n",
    "                      chain_type=\"map_rerank\",\n",
    "                      return_intermediate_steps=True\n",
    "                      ) \n",
    "\n",
    "query = \"who are openai?\"\n",
    "docs = docsearch.similarity_search(query,k=10)\n",
    "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is OpenAI?',\n",
       " 'result': ' OpenAI is an organization dedicated to developing AI technologies with the goal of putting the power of AI directly into the hands of millions of people in order to empower individuals with versatile new tools for their careers, professional development, and economic autonomy.',\n",
       " 'source_documents': [Document(page_content='ing to their own lives however they best saw fit.\\nEspecially in the realm of work, I realized, AI deployed in this \\nway could give individuals incredibly versatile new tools to \\napply to their careers, professional development, and economic \\nautonomy. So when I had a chance to become one of OpenAI’s \\ninitial funders in 2015, I took it. The vision of AI that it was \\nplanning to pursue felt like a natural extension of the goals that \\nhad inspired me to co-found LinkedIn in 2002.\\nWhen OpenAI released its text-to-image generation tool, \\nDALL-E 2, in April 2022, and then followed up six months later \\nwith ChatGPT, the organization’s mission to give millions of \\nusers hands-on access to these remarkable AI tools started to \\nplay out in a big way.\\nNow, thanks to these tools and others like Midjourney and \\nStable Diffusion, a new kind of opt-in, user-driven, and very \\nvisible AI usage suddenly exists. Users share their outputs, \\ntechniques, experiences, and opinions on Twitter, YouTube,', metadata={}),\n",
       "  Document(page_content='sion from Congress.\\nWanting to protect society from bad tech outcomes is not a new \\nphenomenon, of course. In fact, it’s exactly this sentiment that \\nled OpenAI’s founders to create their organization in 2015.\\nSo what’s the most effective and inclusive way to achieve good \\noutcomes for society in the long term?\\nIn recent years, the predominant critique of AI is that it is some-\\nthing that has largely been happening to individuals rather \\nthan for them—an under-the-radar force deployed by Big Tech \\nwithout much public knowledge, much less consent, via tech -\\nnologies like facial recognition and algorithmic decision-mak-\\ning on home loans, job applicant screening, social media rec-\\nommendations, and more.\\nA founding goal of OpenAI was to develop technologies that put \\nthe power of AI directly into the hands of millions of people. \\nIn this way, AI might function as a decentralized, personally \\nempowering force, rather than a top-down, totalizing one.', metadata={}),\n",
       "  Document(page_content='Because, really, an AI book? When things are moving so \\nquickly? Even with a helpful AI on hand to speed the process, \\nany such book would be obsolete before we started to write it—\\nthat’s how fast the industry is moving.\\nSo I hemmed and hawed for a bit. And then I thought of a frame \\nthat pushed me into action.\\nThis didn’t have to be a comprehensive “book” book so much as \\na travelog, an informal exercise in exploration and discovery, \\nme (with GPT-4) choosing one path among many. A snapshot \\nmemorializing—in a subjective and decidedly not definitive \\nway—the AI future we were about to experience.\\nWhat would we see? What would impress us most? What would \\nwe learn about ourselves in the process? Well aware of the brief \\nhalf-life of this travelog’s relevance, I decided to press ahead.\\nA month later, at the end of November 2022, OpenAI released \\nChatGPT, a “conversational agent,” aka chatbot, a modified \\nversion of GPT-3.5 that they had fine-tuned through a process', metadata={}),\n",
       "  Document(page_content='Clearly, I see AI as a transformative force in both my profes -\\nsional and philanthropic work. I anticipate that it will help me \\nbe more efficient, productive, and creative in a variety of ways. \\nI also see the potential for AI to amplify the impact of the orga-\\nnizations I support by helping them connect with a wider audi-\\nence, streamline operations, and identify new opportunities.\\nAs an investor, I’m keenly aware of the potential for AI-pow-\\nered companies to achieve tremendous success, and as the \\nworld increasingly embraces AI, I’m positioning myself and my \\norganizations to be at the forefront of this transformation.\\nI hope you will do the same for you and yours.WHEN AI MAKES THINGS UP  \\n(“HALLUCINATIONS”)\\nWhen OpenAI introduced ChatGPT to the world via a \\n“research preview” on November 30, 2022, a company blog \\npost warned that “ChatGPT sometimes writes plausible-sound -\\ning but incorrect or nonsensical answers.”\\nIn just five days, one million people signed up to give ChatGPT', metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type='similarity', search_kwargs={'k':4})\n",
    "\n",
    "rqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=retriever, return_source_documents=True)\n",
    "rqa('What is OpenAI?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" GPT-4 has the potential to be a powerful tool for creating images, music, video, and other forms of media. It could help human creators with tasks such as brainstorming, editing, feedback, translation, and marketing. Additionally, GPT-4 could be used as a creative assistant, extending the reach of creators' creative powers. However, the impact of GPT-4 on creativity and cultural production is still being explored, and the potential opportunities and challenges remain to be seen.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What dies gpt-4 mean for creativity?'\n",
    "rqa(query)['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The last 20 years have been mostly bad news for the American journalism industry. Newspaper publishers, which have traditionally done the heavy lifting of holding power accountable and informing the public about current affairs, have suffered the worst of it. According to the Pew Research Center, more than 2,200 local U.S. papers have closed since 2005, and over 40,000 newsroom employees have lost their jobs. Competition for ad dollars from non-news players has destroyed the industry’s traditional business models, and participation from a public that, finally empowered to talk back, has slowly eroded journalism’s authority.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What have the last 20 years be like for American journalism?'\n",
    "rqa(query)['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Beagle Bard does not exist. The context mentions GPT-4 and catbot, which are both forms of artificial intelligence, but there is no mention of beagle Bard.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is beagle Bard?'\n",
    "rqa(query)['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
